{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98477057",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0f8b7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r'C:\\Users\\hp\\Documents\\GitHub\\Forecast_Treasury_Curve\\Dataset\\final_feature_library_all_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ec1c3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Spread'] = data['USGG10YR_mean'] - data['USGG2YR_mean']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd5349d",
   "metadata": {},
   "source": [
    "# ARIMA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccef690f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set length: 267\n",
      "Test set length: 36\n",
      "\n",
      "Original Training Series - Augmented Dickey-Fuller Test:\n",
      "ADF Statistic: -3.195596\n",
      "p-value: 0.020241\n",
      "Critical Values:\n",
      "\t1%: -3.455953\n",
      "\t5%: -2.872809\n",
      "\t10%: -2.572775\n",
      "Series is stationary (reject null hypothesis)\n",
      "\n",
      "Total parameter combinations to test: 108\n",
      "\n",
      "================================================================================\n",
      "ARIMA HYPERPARAMETER TUNING - GRID SEARCH\n",
      "================================================================================\n",
      "Order (p,d,q)   AIC        BIC        HQIC       CV MAE     CV Folds   Status\n",
      "-------------------------------------------------------------------------------------\n",
      "(0, 0, 0)       696.64     703.81     699.52     1.5059     31         Success\n",
      "(0, 0, 1)       358.21     368.97     362.53     0.7578     31         Success\n",
      "(0, 0, 2)       112.26     126.61     118.02     0.4389     31         Success\n",
      "(0, 0, 3)       4.81       22.75      12.01      0.3141     31         Success\n",
      "(0, 0, 4)       -94.08     -72.56     -85.44     0.2578     31         Success\n",
      "(0, 0, 5)       -142.65    -117.54    -132.56    0.1995     31         Success\n",
      "(0, 1, 0)       -254.80    -251.22    -253.36    0.1106     31         Success\n",
      "(0, 1, 1)       -305.35    -298.19    -302.47    0.1166     31         Success\n",
      "(0, 1, 2)       -304.84    -294.09    -300.52    0.1141     31         Success\n",
      "(0, 1, 3)       -304.88    -290.55    -299.12    0.1124     31         Success\n",
      "(0, 1, 4)       -305.37    -287.46    -298.17    0.1125     31         Success\n",
      "(0, 1, 5)       -305.86    -284.36    -297.23    0.1133     31         Success\n",
      "(0, 2, 0)       -195.05    -191.47    -193.61    0.1610     31         Success\n",
      "(0, 2, 1)       -260.76    -253.60    -257.88    0.1216     31         Success\n",
      "(0, 2, 2)       -296.44    -285.70    -292.13    0.1180     31         Success\n",
      "(0, 2, 3)       -295.57    -281.25    -289.81    0.1182     31         Success\n",
      "(0, 2, 4)       -295.98    -278.08    -288.79    0.1175     31         Success\n",
      "(0, 2, 5)       -296.65    -275.17    -288.02    0.1135     31         Success\n",
      "(1, 0, 0)       -250.21    -239.45    -245.88    0.1113     31         Success\n",
      "(1, 0, 1)       -302.33    -287.98    -296.56    0.1162     31         Success\n",
      "Progress: 20/108 combinations tested\n",
      "(1, 0, 2)       -301.42    -283.48    -294.22    0.1127     31         Success\n",
      "(1, 0, 3)       -301.93    -280.41    -293.28    0.1120     31         Success\n",
      "(1, 0, 4)       -303.12    -278.01    -293.04    0.1114     31         Success\n",
      "(1, 0, 5)       -303.18    -274.49    -291.66    0.1125     31         Success\n",
      "(1, 1, 0)       -291.64    -284.47    -288.76    0.1162     31         Success\n",
      "(1, 1, 1)       -305.17    -294.42    -300.86    0.1134     31         Success\n",
      "(1, 1, 2)       -303.25    -288.92    -297.49    0.1135     31         Success\n",
      "(1, 1, 3)       -306.44    -288.52    -299.24    0.1106     31         Success\n",
      "(1, 1, 4)       -304.49    -282.98    -295.85    0.1108     31         Success\n",
      "(1, 1, 5)       -302.59    -277.51    -292.52    0.1116     31         Success\n",
      "(1, 2, 0)       -207.73    -200.57    -204.85    0.1514     31         Success\n",
      "(1, 2, 1)       -282.89    -272.15    -278.58    0.1174     31         Success\n",
      "(1, 2, 2)       -295.60    -281.28    -289.85    0.1190     31         Success\n",
      "(1, 2, 3)       -293.70    -275.80    -286.50    0.1187     31         Success\n",
      "(1, 2, 4)       -298.04    -276.56    -289.41    0.1166     31         Success\n",
      "(1, 2, 5)       -299.57    -274.51    -289.50    0.1131     31         Success\n",
      "(2, 0, 0)       -289.15    -274.80    -283.39    0.1170     31         Success\n",
      "(2, 0, 1)       -301.72    -283.79    -294.52    0.1122     31         Success\n",
      "(2, 0, 2)       -303.83    -282.30    -295.18    0.1164     31         Success\n",
      "(2, 0, 3)       -308.95    -283.84    -298.87    0.1069     31         Success\n",
      "Progress: 40/108 combinations tested\n",
      "(2, 0, 4)       -305.84    -277.15    -294.32    0.1108     31         Success\n",
      "(2, 0, 5)       -305.04    -272.76    -292.07    0.1084     31         Success\n",
      "(2, 1, 0)       -293.78    -283.03    -289.46    0.1158     31         Success\n",
      "(2, 1, 1)       -303.42    -289.09    -297.67    0.1138     31         Success\n",
      "(2, 1, 2)       -305.57    -287.65    -298.37    0.1126     31         Success\n",
      "(2, 1, 3)       -300.33    -278.83    -291.70    0.1121     31         Success\n",
      "(2, 1, 4)       -305.96    -280.88    -295.88    0.1113     31         Success\n",
      "(2, 1, 5)       -304.36    -275.69    -292.84    0.1158     31         Success\n",
      "(2, 2, 0)       -262.90    -252.16    -258.59    0.1215     31         Success\n",
      "(2, 2, 1)       -285.90    -271.58    -280.15    0.1193     31         Success\n",
      "(2, 2, 2)       -293.85    -275.95    -286.66    0.1181     31         Success\n",
      "(2, 2, 3)       -296.13    -274.65    -287.50    0.1182     31         Success\n",
      "(2, 2, 4)       -295.05    -269.99    -284.98    0.1169     31         Success\n",
      "(2, 2, 5)       -297.46    -268.82    -285.95    0.1176     31         Success\n",
      "(3, 0, 0)       -290.45    -272.51    -283.25    0.1171     31         Success\n",
      "(3, 0, 1)       -300.19    -278.66    -291.54    0.1134     31         Success\n",
      "(3, 0, 2)       -307.87    -282.76    -297.79    0.1088     31         Success\n",
      "(3, 0, 3)       -302.83    -274.13    -291.30    0.1161     31         Success\n",
      "(3, 0, 4)       -303.88    -271.60    -290.91    0.1112     31         Success\n",
      "(3, 0, 5)       -306.76    -270.89    -292.35    0.1100     31         Success\n",
      "Progress: 60/108 combinations tested\n",
      "(3, 1, 0)       -304.58    -290.25    -298.82    0.1118     31         Success\n",
      "(3, 1, 1)       -306.33    -288.41    -299.13    0.1115     31         Success\n",
      "(3, 1, 2)       -308.83    -287.33    -300.20    0.1107     31         Success\n",
      "(3, 1, 3)       -308.74    -283.66    -298.67    0.1129     31         Success\n",
      "(3, 1, 4)       -302.14    -273.48    -290.63    0.1142     31         Success\n",
      "(3, 1, 5)       -303.10    -270.85    -290.14    0.1122     31         Success\n",
      "(3, 2, 0)       -262.32    -248.00    -256.57    0.1206     31         Success\n",
      "(3, 2, 1)       -296.05    -278.15    -288.86    0.1125     31         Success\n",
      "(3, 2, 2)       -297.66    -276.18    -289.03    0.1124     31         Success\n",
      "(3, 2, 3)       -299.97    -274.91    -289.90    0.1165     31         Success\n",
      "(3, 2, 4)       -298.44    -269.80    -286.93    0.1152     31         Success\n",
      "(3, 2, 5)       -296.18    -263.96    -283.23    0.1148     31         Success\n",
      "(4, 0, 0)       -303.19    -281.67    -294.54    0.1095     31         Success\n",
      "(4, 0, 1)       -304.33    -279.22    -294.24    0.1103     31         Success\n",
      "(4, 0, 2)       -305.59    -276.89    -294.06    0.1081     31         Success\n",
      "(4, 0, 3)       -304.34    -272.05    -291.37    0.1146     31         Success\n",
      "(4, 0, 4)       -300.05    -264.18    -285.64    0.1146     31         Success\n",
      "(4, 0, 5)       -304.42    -264.96    -288.57    0.1092     31         Success\n",
      "(4, 1, 0)       -305.10    -287.19    -297.91    0.1127     31         Success\n",
      "(4, 1, 1)       -304.40    -282.90    -295.76    0.1117     31         Success\n",
      "Progress: 80/108 combinations tested\n",
      "(4, 1, 2)       -306.60    -281.51    -296.52    0.1109     31         Success\n",
      "(4, 1, 3)       -305.77    -277.11    -294.26    0.1118     31         Success\n",
      "(4, 1, 4)       -304.49    -272.24    -291.54    0.1084     31         Success\n",
      "(4, 1, 5)       -307.39    -271.55    -292.99    0.1132     31         Success\n",
      "(4, 2, 0)       -275.94    -258.04    -268.75    0.1201     31         Success\n",
      "(4, 2, 1)       -294.19    -272.72    -285.56    0.1200     31         Success\n",
      "(4, 2, 2)       -295.70    -270.64    -285.63    0.1133     31         Success\n",
      "(4, 2, 3)       -295.11    -266.47    -283.60    0.1155     31         Success\n",
      "(4, 2, 4)       -296.47    -264.25    -283.52    0.1149     31         Success\n",
      "(4, 2, 5)       -296.33    -260.53    -281.94    0.1255     31         Success\n",
      "(5, 0, 0)       -302.89    -277.78    -292.80    0.1105     31         Success\n",
      "(5, 0, 1)       -302.33    -273.63    -290.80    0.1098     31         Success\n",
      "(5, 0, 2)       -301.43    -269.15    -288.46    0.1047     31         Success\n",
      "(5, 0, 3)       -303.33    -267.46    -288.92    0.1122     31         Success\n",
      "(5, 0, 4)       -299.21    -259.75    -283.36    0.1129     31         Success\n",
      "(5, 0, 5)       -306.37    -263.32    -289.08    0.1093     31         Success\n",
      "(5, 1, 0)       -305.65    -284.15    -297.01    0.1110     31         Success\n",
      "(5, 1, 1)       -306.30    -281.21    -296.22    0.1126     31         Success\n",
      "(5, 1, 2)       -305.06    -276.39    -293.54    0.1141     31         Success\n",
      "(5, 1, 3)       -302.74    -270.49    -289.78    0.1133     31         Success\n",
      "Progress: 100/108 combinations tested\n",
      "(5, 1, 4)       -303.42    -267.58    -289.02    0.1163     31         Success\n",
      "(5, 1, 5)       -305.77    -266.36    -289.94    0.1137     31         Success\n",
      "(5, 2, 0)       -280.46    -258.98    -271.83    0.1207     31         Success\n",
      "(5, 2, 1)       -297.12    -272.06    -287.05    0.1153     31         Success\n",
      "(5, 2, 2)       -292.36    -263.73    -280.86    0.1143     31         Success\n",
      "(5, 2, 3)       -270.97    -238.75    -258.02    0.1199     31         Success\n",
      "(5, 2, 4)       -298.72    -262.92    -284.34    0.1245     31         Success\n",
      "(5, 2, 5)       -294.97    -255.59    -279.14    0.1247     31         Success\n",
      "\n",
      "================================================================================\n",
      "BEST MODELS BY DIFFERENT CRITERIA:\n",
      "================================================================================\n",
      "Best by AIC: (2, 0, 3) (AIC: -308.95)\n",
      "Best by BIC: (0, 1, 1) (BIC: -298.19)\n",
      "Best by CV MAE: (5, 0, 2) (CV MAE: 0.1047)\n",
      "\n",
      "================================================================================\n",
      "TOP 5 MODELS BY AIC:\n",
      "================================================================================\n",
      "    order         AIC         BIC   CV_MAE\n",
      "(2, 0, 3) -308.954482 -283.843742 0.106874\n",
      "(3, 1, 2) -308.834316 -287.333338 0.110709\n",
      "(3, 1, 3) -308.742685 -283.658210 0.112907\n",
      "(3, 0, 2) -307.873715 -282.762974 0.108838\n",
      "(4, 1, 5) -307.389187 -271.554224 0.113200\n",
      "\n",
      "================================================================================\n",
      "TOP 5 MODELS BY BIC:\n",
      "================================================================================\n",
      "    order         AIC         BIC   CV_MAE\n",
      "(0, 1, 1) -305.352271 -298.185278 0.116647\n",
      "(1, 1, 1) -305.174236 -294.423747 0.113400\n",
      "(0, 1, 2) -304.837893 -294.087404 0.114071\n",
      "(0, 1, 3) -304.881801 -290.547816 0.112446\n",
      "(3, 1, 0) -304.580552 -290.246567 0.111841\n",
      "\n",
      "================================================================================\n",
      "TOP 5 MODELS BY CV MAE:\n",
      "================================================================================\n",
      "    order         AIC         BIC   CV_MAE\n",
      "(5, 0, 2) -301.432296 -269.147059 0.104715\n",
      "(2, 0, 3) -308.954482 -283.843742 0.106874\n",
      "(4, 0, 2) -305.587831 -276.889842 0.108092\n",
      "(4, 1, 4) -304.492815 -272.241349 0.108399\n",
      "(2, 0, 5) -305.041244 -272.756006 0.108435\n",
      "\n",
      "================================================================================\n",
      "FINAL SELECTED MODEL:\n",
      "================================================================================\n",
      "Selected ARIMA order: (5, 0, 2)\n",
      "AIC: -301.43\n",
      "BIC: -269.15\n",
      "CV MAE: 0.1047\n",
      "Number of CV folds: 31\n",
      "\n",
      "Model Summary:\n",
      "                               SARIMAX Results                                \n",
      "==============================================================================\n",
      "Dep. Variable:                 Spread   No. Observations:                  267\n",
      "Model:                 ARIMA(5, 0, 2)   Log Likelihood                 159.716\n",
      "Date:                Fri, 20 Jun 2025   AIC                           -301.432\n",
      "Time:                        20:34:14   BIC                           -269.147\n",
      "Sample:                             0   HQIC                          -288.464\n",
      "                                - 267                                         \n",
      "Covariance Type:                  opg                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          1.1943      0.267      4.470      0.000       0.671       1.718\n",
      "ar.L1          1.4935      0.815      1.833      0.067      -0.104       3.090\n",
      "ar.L2         -0.4699      1.025     -0.458      0.647      -2.480       1.540\n",
      "ar.L3          0.1178      0.451      0.261      0.794      -0.766       1.002\n",
      "ar.L4         -0.1826      0.379     -0.482      0.630      -0.925       0.559\n",
      "ar.L5          0.0216      0.240      0.090      0.928      -0.449       0.492\n",
      "ma.L1         -0.0509      0.826     -0.062      0.951      -1.671       1.569\n",
      "ma.L2         -0.3142      0.269     -1.170      0.242      -0.841       0.212\n",
      "sigma2         0.0174      0.001     13.511      0.000       0.015       0.020\n",
      "===================================================================================\n",
      "Ljung-Box (L1) (Q):                   0.03   Jarque-Bera (JB):                94.69\n",
      "Prob(Q):                              0.87   Prob(JB):                         0.00\n",
      "Heteroskedasticity (H):               0.77   Skew:                             0.07\n",
      "Prob(H) (two-sided):                  0.23   Kurtosis:                         5.91\n",
      "===================================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Covariance matrix calculated using the outer product of gradients (complex-step).\n",
      "\n",
      "================================================================================\n",
      "TEST SET PERFORMANCE:\n",
      "================================================================================\n",
      "Test MAE: 0.9864\n",
      "Test RMSE: 1.0646\n",
      "\n",
      "First 10 Test Forecasts:\n",
      "     Actual  Forecast   Error\n",
      "267  0.2242    0.1965  0.0277\n",
      "268  0.2851    0.1752  0.1099\n",
      "269  0.1302    0.1452  0.0150\n",
      "270 -0.1396    0.1513  0.2909\n",
      "271 -0.3536    0.1707  0.5243\n",
      "272 -0.3401    0.1967  0.5368\n",
      "273 -0.4053    0.2321  0.6373\n",
      "274 -0.6286    0.2732  0.9018\n",
      "275 -0.6782    0.3177  0.9959\n",
      "276 -0.6889    0.3647  1.0536\n",
      "\n",
      "ARIMA hyperparameter tuning completed!\n",
      "Best model: ARIMA(5, 0, 2)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Assuming 'data' is your DataFrame with 'Spread' column\n",
    "# Define test size\n",
    "test_size = 36\n",
    "\n",
    "# Split data into train and test\n",
    "train_data = data[:-test_size].copy()\n",
    "test_data = data[-test_size:].copy()\n",
    "train_series = train_data['Spread']\n",
    "test_series = test_data['Spread']\n",
    "\n",
    "print(f\"Training set length: {len(train_data)}\")\n",
    "print(f\"Test set length: {len(test_data)}\")\n",
    "\n",
    "# Check stationarity\n",
    "def check_stationarity(series, series_name=\"Series\"):\n",
    "    \"\"\"\n",
    "    Perform Augmented Dickey-Fuller test to check stationarity\n",
    "    \"\"\"\n",
    "    result = adfuller(series.dropna())\n",
    "    print(f'\\n{series_name} - Augmented Dickey-Fuller Test:')\n",
    "    print(f'ADF Statistic: {result[0]:.6f}')\n",
    "    print(f'p-value: {result[1]:.6f}')\n",
    "    print(f'Critical Values:')\n",
    "    for key, value in result[4].items():\n",
    "        print(f'\\t{key}: {value:.6f}')\n",
    "    \n",
    "    if result[1] <= 0.05:\n",
    "        print(\"Series is stationary (reject null hypothesis)\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"Series is non-stationary (fail to reject null hypothesis)\")\n",
    "        return False\n",
    "\n",
    "# Check stationarity of original series\n",
    "is_stationary = check_stationarity(train_series, \"Original Training Series\")\n",
    "\n",
    "# Define parameter ranges for grid search\n",
    "p_values = range(0, 6)  # AR order\n",
    "d_values = range(0, 3)  # Differencing order\n",
    "q_values = range(0, 6)  # MA order\n",
    "\n",
    "# Create all combinations\n",
    "param_combinations = list(product(p_values, d_values, q_values))\n",
    "print(f\"\\nTotal parameter combinations to test: {len(param_combinations)}\")\n",
    "\n",
    "def fit_arima_model(series, order):\n",
    "    \"\"\"\n",
    "    Fit ARIMA model with given order\n",
    "    Returns fitted model or None if fitting fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        model = ARIMA(series, order=order)\n",
    "        fitted_model = model.fit()\n",
    "        return fitted_model\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def calculate_information_criteria(fitted_model):\n",
    "    \"\"\"\n",
    "    Calculate AIC, BIC, and HQIC for model selection\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return {\n",
    "            'AIC': fitted_model.aic,\n",
    "            'BIC': fitted_model.bic,\n",
    "            'HQIC': fitted_model.hqic\n",
    "        }\n",
    "    except:\n",
    "        return {\n",
    "            'AIC': np.inf,\n",
    "            'BIC': np.inf,\n",
    "            'HQIC': np.inf\n",
    "        }\n",
    "\n",
    "def arima_time_series_cv(train_data, test_data, order, min_train_size=30):\n",
    "    \"\"\"\n",
    "    Perform time series cross-validation for ARIMA model\n",
    "    \"\"\"\n",
    "    cv_scores = []\n",
    "    n_test = len(test_data)\n",
    "    \n",
    "    # Combine train and test data\n",
    "    full_series = pd.concat([train_data, test_data])\n",
    "    train_end_idx = len(train_data)\n",
    "    \n",
    "    for i in range(min_train_size, n_test):\n",
    "        try:\n",
    "            # Use training data + first i test observations\n",
    "            cv_train_series = full_series.iloc[:train_end_idx + i]\n",
    "            \n",
    "            # Fit ARIMA model\n",
    "            model = ARIMA(cv_train_series, order=order)\n",
    "            fitted_model = model.fit()\n",
    "            \n",
    "            # Forecast next observation\n",
    "            forecast = fitted_model.forecast(steps=1)\n",
    "            cv_val_actual = test_data.iloc[i]\n",
    "            cv_val_pred = forecast.iloc[0] if hasattr(forecast, 'iloc') else forecast[0]\n",
    "            \n",
    "            mae_fold = abs(cv_val_actual - cv_val_pred)\n",
    "            cv_scores.append(mae_fold)\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Skip this fold if model fitting fails\n",
    "            continue\n",
    "    \n",
    "    return cv_scores\n",
    "\n",
    "# Hyperparameter tuning\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"ARIMA HYPERPARAMETER TUNING - GRID SEARCH\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "results = []\n",
    "best_aic = np.inf\n",
    "best_bic = np.inf\n",
    "best_cv_mae = np.inf\n",
    "best_params_aic = None\n",
    "best_params_bic = None\n",
    "best_params_cv = None\n",
    "\n",
    "print(f\"{'Order (p,d,q)':<15} {'AIC':<10} {'BIC':<10} {'HQIC':<10} {'CV MAE':<10} {'CV Folds':<10} {'Status'}\")\n",
    "print(f\"{'-'*85}\")\n",
    "\n",
    "for i, (p, d, q) in enumerate(param_combinations):\n",
    "    order = (p, d, q)\n",
    "    \n",
    "    # Fit model on training data\n",
    "    fitted_model = fit_arima_model(train_series, order)\n",
    "    \n",
    "    if fitted_model is not None:\n",
    "        # Calculate information criteria\n",
    "        ic_scores = calculate_information_criteria(fitted_model)\n",
    "        \n",
    "        # Perform cross-validation\n",
    "        cv_scores = arima_time_series_cv(train_series, test_series, order, min_train_size=5)\n",
    "        \n",
    "        if len(cv_scores) > 0:\n",
    "            mean_cv_mae = np.mean(cv_scores)\n",
    "            n_cv_folds = len(cv_scores)\n",
    "            status = \"Success\"\n",
    "            \n",
    "            # Track best models\n",
    "            if ic_scores['AIC'] < best_aic:\n",
    "                best_aic = ic_scores['AIC']\n",
    "                best_params_aic = order\n",
    "            \n",
    "            if ic_scores['BIC'] < best_bic:\n",
    "                best_bic = ic_scores['BIC']\n",
    "                best_params_bic = order\n",
    "            \n",
    "            if mean_cv_mae < best_cv_mae:\n",
    "                best_cv_mae = mean_cv_mae\n",
    "                best_params_cv = order\n",
    "            \n",
    "            results.append({\n",
    "                'order': order,\n",
    "                'AIC': ic_scores['AIC'],\n",
    "                'BIC': ic_scores['BIC'],\n",
    "                'HQIC': ic_scores['HQIC'],\n",
    "                'CV_MAE': mean_cv_mae,\n",
    "                'CV_Folds': n_cv_folds,\n",
    "                'fitted_model': fitted_model\n",
    "            })\n",
    "            \n",
    "            print(f\"{str(order):<15} {ic_scores['AIC']:<10.2f} {ic_scores['BIC']:<10.2f} \"\n",
    "                  f\"{ic_scores['HQIC']:<10.2f} {mean_cv_mae:<10.4f} {n_cv_folds:<10} {status}\")\n",
    "        \n",
    "        else:\n",
    "            print(f\"{str(order):<15} {ic_scores['AIC']:<10.2f} {ic_scores['BIC']:<10.2f} \"\n",
    "                  f\"{ic_scores['HQIC']:<10.2f} {'No CV':<10} {'0':<10} No CV\")\n",
    "    \n",
    "    else:\n",
    "        print(f\"{str(order):<15} {'Failed':<10} {'Failed':<10} {'Failed':<10} {'Failed':<10} {'0':<10} Failed\")\n",
    "    \n",
    "    # Progress indicator\n",
    "    if (i + 1) % 20 == 0:\n",
    "        print(f\"Progress: {i + 1}/{len(param_combinations)} combinations tested\")\n",
    "\n",
    "# Sort results by different criteria\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "if len(results_df) > 0:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"BEST MODELS BY DIFFERENT CRITERIA:\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Best by AIC\n",
    "    best_aic_result = results_df.loc[results_df['AIC'].idxmin()]\n",
    "    print(f\"Best by AIC: {best_aic_result['order']} (AIC: {best_aic_result['AIC']:.2f})\")\n",
    "    \n",
    "    # Best by BIC\n",
    "    best_bic_result = results_df.loc[results_df['BIC'].idxmin()]\n",
    "    print(f\"Best by BIC: {best_bic_result['order']} (BIC: {best_bic_result['BIC']:.2f})\")\n",
    "    \n",
    "    # Best by CV MAE\n",
    "    best_cv_result = results_df.loc[results_df['CV_MAE'].idxmin()]\n",
    "    print(f\"Best by CV MAE: {best_cv_result['order']} (CV MAE: {best_cv_result['CV_MAE']:.4f})\")\n",
    "    \n",
    "    # Top 5 models by each criterion\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"TOP 5 MODELS BY AIC:\")\n",
    "    print(f\"{'='*80}\")\n",
    "    top_aic = results_df.nsmallest(5, 'AIC')[['order', 'AIC', 'BIC', 'CV_MAE']]\n",
    "    print(top_aic.to_string(index=False))\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"TOP 5 MODELS BY BIC:\")\n",
    "    print(f\"{'='*80}\")\n",
    "    top_bic = results_df.nsmallest(5, 'BIC')[['order', 'AIC', 'BIC', 'CV_MAE']]\n",
    "    print(top_bic.to_string(index=False))\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"TOP 5 MODELS BY CV MAE:\")\n",
    "    print(f\"{'='*80}\")\n",
    "    top_cv = results_df.nsmallest(5, 'CV_MAE')[['order', 'AIC', 'BIC', 'CV_MAE']]\n",
    "    print(top_cv.to_string(index=False))\n",
    "    \n",
    "    # Final model selection (prioritize CV performance)\n",
    "    final_best_order = best_cv_result['order']\n",
    "    final_best_model = best_cv_result['fitted_model']\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"FINAL SELECTED MODEL:\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Selected ARIMA order: {final_best_order}\")\n",
    "    print(f\"AIC: {best_cv_result['AIC']:.2f}\")\n",
    "    print(f\"BIC: {best_cv_result['BIC']:.2f}\")\n",
    "    print(f\"CV MAE: {best_cv_result['CV_MAE']:.4f}\")\n",
    "    print(f\"Number of CV folds: {best_cv_result['CV_Folds']}\")\n",
    "    \n",
    "    # Model summary\n",
    "    print(f\"\\nModel Summary:\")\n",
    "    print(final_best_model.summary())\n",
    "    \n",
    "    # Generate forecasts for test set\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"TEST SET PERFORMANCE:\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Forecast test set\n",
    "    test_forecasts = final_best_model.forecast(steps=test_size)\n",
    "    test_mae = mean_absolute_error(test_series, test_forecasts)\n",
    "    test_rmse = np.sqrt(mean_squared_error(test_series, test_forecasts))\n",
    "    \n",
    "    print(f\"Test MAE: {test_mae:.4f}\")\n",
    "    print(f\"Test RMSE: {test_rmse:.4f}\")\n",
    "    \n",
    "    # Show first 10 forecasts\n",
    "    print(f\"\\nFirst 10 Test Forecasts:\")\n",
    "    forecast_comparison = pd.DataFrame({\n",
    "        'Actual': test_series.iloc[:10].values,\n",
    "        'Forecast': test_forecasts[:10],\n",
    "        'Error': np.abs(test_series.iloc[:10].values - test_forecasts[:10])\n",
    "    })\n",
    "    print(forecast_comparison.round(4))\n",
    "    \n",
    "    # Function to get the best model\n",
    "    def get_best_arima_model():\n",
    "        return final_best_model, final_best_order\n",
    "    \n",
    "    # Save results for later use\n",
    "    arima_results = {\n",
    "        'best_model': final_best_model,\n",
    "        'best_order': final_best_order,\n",
    "        'test_mae': test_mae,\n",
    "        'test_rmse': test_rmse,\n",
    "        'all_results': results_df\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nARIMA hyperparameter tuning completed!\")\n",
    "    print(f\"Best model: ARIMA{final_best_order}\")\n",
    "    \n",
    "else:\n",
    "    print(\"No successful ARIMA models fitted. Check your data and parameter ranges.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
