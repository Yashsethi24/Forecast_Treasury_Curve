{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60d48a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c094cd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "def time_series_mae(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate Mean Absolute Error (MAE) for time series cross validation.\n",
    "\n",
    "    Parameters:\n",
    "    y_true (array-like): Original/actual values.\n",
    "    y_pred (array-like): Model predicted values.\n",
    "\n",
    "    Returns:\n",
    "    float: MAE value.\n",
    "    \"\"\"\n",
    "    return mean_absolute_error(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92cb3459",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r'C:\\Users\\hp\\Documents\\GitHub\\Forecast_Treasury_Curve\\Dataset\\final_feature_library_all_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e31034e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Spread'] = data['USGG10YR_mean'] - data['USGG2YR_mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "671f88c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set length: 267\n",
      "Test set length: 36\n"
     ]
    }
   ],
   "source": [
    "# Split the time series into training and test sets (e.g., last 12 months as test)\n",
    "test_size = 36\n",
    "train = data['Spread'][:-test_size]\n",
    "test = data['Spread'][-test_size:]\n",
    "\n",
    "print(f\"Training set length: {len(train)}\")\n",
    "print(f\"Test set length: {len(test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef764d4",
   "metadata": {},
   "source": [
    "# Naive method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "902addc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original training set length: 267\n",
      "Original test set length: 36\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "data = data[['Date', 'USGG10YR_mean', 'USGG2YR_mean', 'Spread']].copy()\n",
    "# Assuming 'data' is your DataFrame with 'Spread' column\n",
    "# Define test size\n",
    "test_size = 36\n",
    "\n",
    "# Split data into train and test BEFORE creating lagged features\n",
    "train_data = data[:-test_size].copy()\n",
    "test_data = data[-test_size:].copy()\n",
    "\n",
    "print(f\"Original training set length: {len(train_data)}\")\n",
    "print(f\"Original test set length: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0528e903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set length after creating lag and dropping NaN: 266\n"
     ]
    }
   ],
   "source": [
    "# Create lagged features for training data\n",
    "train_data['Spread_lag1'] = train_data['Spread'].shift(1)\n",
    "# Remove the first row with NaN after shift\n",
    "train_data = train_data.dropna()\n",
    "\n",
    "print(f\"Training set length after creating lag and dropping NaN: {len(train_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89319f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set length: 36\n"
     ]
    }
   ],
   "source": [
    "# For test set, we need to be careful about the lag feature\n",
    "# The first prediction in test set should use the last value from training set\n",
    "test_data['Spread_lag1'] = test_data['Spread'].shift(1)\n",
    "\n",
    "# Fill the first NaN with the last value from training data\n",
    "test_data.iloc[0, test_data.columns.get_loc('Spread_lag1')] = train_data['Spread'].iloc[-1]\n",
    "\n",
    "print(f\"Test set length: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73d04fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Mean Absolute Error (MAE): 0.11473869141727826\n",
      "\n",
      "Time Series Cross-Validation Results:\n",
      "Number of CV folds: 31\n",
      "Mean CV MAE: 0.11061027686198786\n",
      "CV MAE Standard Deviation: 0.08983990502011888\n",
      "\n",
      "==================================================\n",
      "SUMMARY OF RESULTS:\n",
      "==================================================\n",
      "Single Test Set MAE: 0.1147\n",
      "Time Series CV MAE: 0.1106\n"
     ]
    }
   ],
   "source": [
    "# Calculate MAE on test set\n",
    "test_actual = test_data['Spread']\n",
    "test_pred = test_data['Spread_lag1']\n",
    "mae_test = mean_absolute_error(test_actual, test_pred)\n",
    "print(f\"Test Set Mean Absolute Error (MAE): {mae_test}\")\n",
    "\n",
    "# Time Series Cross-Validation on Test Set\n",
    "def time_series_cv_on_test(test_data, min_train_size=5, step=1):\n",
    "    \"\"\"\n",
    "    Perform time series cross-validation on the test set\n",
    "    \"\"\"\n",
    "    cv_scores = []\n",
    "    n_test = len(test_data)\n",
    "    \n",
    "    for i in range(min_train_size, n_test, step):\n",
    "        # Use first i observations as \"training\" within test set\n",
    "        cv_train = test_data.iloc[:i]\n",
    "        # Use next observation as \"validation\"\n",
    "        if i < n_test:\n",
    "            cv_val_actual = test_data.iloc[i]['Spread']\n",
    "            cv_val_pred = test_data.iloc[i-1]['Spread']  # Naive forecast\n",
    "            \n",
    "            mae_fold = abs(cv_val_actual - cv_val_pred)\n",
    "            cv_scores.append(mae_fold)\n",
    "    \n",
    "    return cv_scores\n",
    "\n",
    "# Perform cross-validation on test set\n",
    "cv_scores = time_series_cv_on_test(test_data, min_train_size=5, step=1)\n",
    "mean_cv_mae = np.mean(cv_scores)\n",
    "\n",
    "print(f\"\\nTime Series Cross-Validation Results:\")\n",
    "print(f\"Number of CV folds: {len(cv_scores)}\")\n",
    "print(f\"Mean CV MAE: {mean_cv_mae}\")\n",
    "print(f\"CV MAE Standard Deviation: {np.std(cv_scores)}\")\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"SUMMARY OF RESULTS:\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Single Test Set MAE: {mae_test:.4f}\")\n",
    "print(f\"Time Series CV MAE: {mean_cv_mae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d28f94",
   "metadata": {},
   "source": [
    "# Drift-method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5406a7d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set length: 267\n",
      "Test set length: 36\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Assuming 'data' is your DataFrame with 'Spread' column\n",
    "# Define test size\n",
    "test_size = 36\n",
    "\n",
    "# Split data into train and test\n",
    "train_data = data[:-test_size].copy()\n",
    "test_data = data[-test_size:].copy()\n",
    "\n",
    "print(f\"Training set length: {len(train_data)}\")\n",
    "print(f\"Test set length: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ebbbd6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drift_forecast(train_series, h=1):\n",
    "    \"\"\"\n",
    "    Calculate drift forecast for h steps ahead\n",
    "    Drift method: y_t+h = y_t + h * (y_T - y_1)/(T-1)\n",
    "    where T is the length of training data\n",
    "    \"\"\"\n",
    "    y_first = train_series.iloc[0]\n",
    "    y_last = train_series.iloc[-1]\n",
    "    T = len(train_series)\n",
    "    \n",
    "    # Calculate drift (average change per period)\n",
    "    drift = (y_last - y_first) / (T - 1)\n",
    "    \n",
    "    # Forecast h steps ahead\n",
    "    forecast = y_last + h * drift\n",
    "    \n",
    "    return forecast, drift\n",
    "\n",
    "# Calculate drift forecasts for test set\n",
    "train_spread = train_data['Spread']\n",
    "test_predictions = []\n",
    "drift_values = []\n",
    "\n",
    "# Create results DataFrame\n",
    "test_results = test_data.copy()\n",
    "\n",
    "# Method 2: Rolling Drift Forecasts (updating drift as we get new data)\n",
    "def rolling_drift_forecast(train_data, test_data):\n",
    "    \"\"\"\n",
    "    Perform rolling drift forecasts where we update our drift estimate\n",
    "    as we move through the test set\n",
    "    \"\"\"\n",
    "    rolling_predictions = []\n",
    "    current_train = train_data['Spread'].copy()\n",
    "    \n",
    "    for i in range(len(test_data)):\n",
    "        # Calculate drift forecast 1-step ahead\n",
    "        forecast, drift = drift_forecast(current_train, h=1)\n",
    "        rolling_predictions.append(forecast)\n",
    "        \n",
    "        # Add actual value to training data for next iteration\n",
    "        if i < len(test_data) - 1:  # Don't add the last value\n",
    "            current_train = pd.concat([current_train, test_data['Spread'].iloc[i:i+1]])\n",
    "    \n",
    "    return rolling_predictions\n",
    "\n",
    "# Rolling drift forecasts\n",
    "rolling_drift_preds = rolling_drift_forecast(train_data, test_data)\n",
    "test_results['Rolling_Drift_Forecast'] = rolling_drift_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "108f1a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rolling Drift Method - Test Set MAE: 0.1146\n"
     ]
    }
   ],
   "source": [
    "# Calculate MAE for rolling drift method\n",
    "mae_rolling_drift = mean_absolute_error(test_results['Spread'], test_results['Rolling_Drift_Forecast'])\n",
    "print(f\"Rolling Drift Method - Test Set MAE: {mae_rolling_drift:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a11046f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Drift Method - CV on Test Set Results:\n",
      "Number of CV folds: 31\n",
      "Mean CV MAE: 0.1104\n",
      "CV MAE Standard Deviation: 0.0907\n"
     ]
    }
   ],
   "source": [
    "# Time Series Cross-Validation for Drift Method\n",
    "def drift_cv_on_test(test_data, train_data, min_train_size=5):\n",
    "    \"\"\"\n",
    "    Perform time series cross-validation on test set for drift method\n",
    "    \"\"\"\n",
    "    cv_scores = []\n",
    "    n_test = len(test_data)\n",
    "    \n",
    "    # Combine train and test for progressive training\n",
    "    full_data = pd.concat([train_data, test_data])\n",
    "    train_end_idx = len(train_data)\n",
    "    \n",
    "    for i in range(min_train_size, n_test):\n",
    "        # Use training data + first i test observations\n",
    "        cv_train_series = full_data['Spread'].iloc[:train_end_idx + i]\n",
    "        \n",
    "        # Predict next test observation\n",
    "        cv_val_actual = test_data['Spread'].iloc[i]\n",
    "        cv_val_pred, _ = drift_forecast(cv_train_series, h=1)\n",
    "        \n",
    "        mae_fold = abs(cv_val_actual - cv_val_pred)\n",
    "        cv_scores.append(mae_fold)\n",
    "    \n",
    "    return cv_scores\n",
    "\n",
    "# CV on test set for drift method\n",
    "drift_test_cv_scores = drift_cv_on_test(test_data, train_data, min_train_size=5)\n",
    "mean_drift_test_cv_mae = np.mean(drift_test_cv_scores)\n",
    "\n",
    "print(f\"\\nDrift Method - CV on Test Set Results:\")\n",
    "print(f\"Number of CV folds: {len(drift_test_cv_scores)}\")\n",
    "print(f\"Mean CV MAE: {mean_drift_test_cv_mae:.4f}\")\n",
    "print(f\"CV MAE Standard Deviation: {np.std(drift_test_cv_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83404ddd",
   "metadata": {},
   "source": [
    "# Moving Averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0386020c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set length: 267\n",
      "Test set length: 36\n",
      "\n",
      "======================================================================\n",
      "MOVING AVERAGE - TIME SERIES CROSS-VALIDATION RESULTS:\n",
      "======================================================================\n",
      "Window Length   Mean CV MAE  Std CV MAE   # CV Folds\n",
      "----------------------------------------------------------------------\n",
      "2               0.1307       0.1021       31        \n",
      "3               0.1476       0.1167       31        \n",
      "4               0.1733       0.1248       31        \n",
      "5               0.1974       0.1308       31        \n",
      "6               0.2167       0.1446       31        \n",
      "8               0.2671       0.1687       31        \n",
      "10              0.3228       0.1910       31        \n",
      "12              0.3802       0.2197       31        \n",
      "15              0.4651       0.2677       31        \n",
      "20              0.5724       0.3770       31        \n",
      "24              0.6290       0.4340       31        \n",
      "30              0.7052       0.4174       31        \n",
      "\n",
      "======================================================================\n",
      "OPTIMAL MOVING AVERAGE CONFIGURATION:\n",
      "======================================================================\n",
      "Optimal window length: 2\n",
      "Optimal CV MAE: 0.1307\n",
      "Optimal CV MAE Std: 0.1021\n",
      "Number of CV folds: 31\n",
      "\n",
      "Test Set Performance with Optimal Window:\n",
      "Test MAE: 0.1409\n",
      "\n",
      "Detailed CV Results for Optimal Window (Length = 2):\n",
      "Individual CV MAE scores: ['0.0935', '0.0584', '0.2559', '0.1612', '0.0355', '0.1217', '0.0994', '0.1583', '0.0234', '0.3352']\n",
      "... and 21 more\n",
      "\n",
      "======================================================================\n",
      "PERFORMANCE RANKING (by CV MAE):\n",
      "======================================================================\n",
      "Rank  Window   CV MAE     Std MAE    # Folds \n",
      "----------------------------------------------------------------------\n",
      "1     2        0.1307     0.1021     31      \n",
      "2     3        0.1476     0.1167     31      \n",
      "3     4        0.1733     0.1248     31      \n",
      "4     5        0.1974     0.1308     31      \n",
      "5     6        0.2167     0.1446     31      \n",
      "6     8        0.2671     0.1687     31      \n",
      "7     10       0.3228     0.1910     31      \n",
      "8     12       0.3802     0.2197     31      \n",
      "9     15       0.4651     0.2677     31      \n",
      "10    20       0.5724     0.3770     31      \n",
      "11    24       0.6290     0.4340     31      \n",
      "12    30       0.7052     0.4174     31      \n",
      "\n",
      "======================================================================\n",
      "ANALYSIS SUMMARY:\n",
      "======================================================================\n",
      "Best 3 window lengths: [2, 3, 4]\n",
      "Worst 3 window lengths: [20, 24, 30]\n",
      "Performance improvement (best vs worst): 81.46%\n",
      "Best CV MAE: 0.1307\n",
      "Worst CV MAE: 0.7052\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Assuming 'data' is your DataFrame with 'Spread' column\n",
    "# Define test size\n",
    "test_size = 36\n",
    "\n",
    "# Split data into train and test\n",
    "train_data = data[:-test_size].copy()\n",
    "test_data = data[-test_size:].copy()\n",
    "\n",
    "print(f\"Training set length: {len(train_data)}\")\n",
    "print(f\"Test set length: {len(test_data)}\")\n",
    "\n",
    "def moving_average_forecast(series, window_length):\n",
    "    \"\"\"\n",
    "    Calculate moving average forecast\n",
    "    Returns the average of the last 'window_length' values\n",
    "    \"\"\"\n",
    "    if len(series) < window_length:\n",
    "        # If not enough data, use all available data\n",
    "        return series.mean()\n",
    "    \n",
    "    return series.iloc[-window_length:].mean()\n",
    "\n",
    "def moving_average_cv_on_test(test_data, train_data, window_length, min_train_size=5):\n",
    "    \"\"\"\n",
    "    Perform time series cross-validation on test set for moving average method\n",
    "    This progressively uses more data from the test set for training\n",
    "    \"\"\"\n",
    "    cv_scores = []\n",
    "    n_test = len(test_data)\n",
    "    \n",
    "    # Combine train and test data for progressive training\n",
    "    full_data = pd.concat([train_data, test_data])\n",
    "    train_end_idx = len(train_data)\n",
    "    \n",
    "    for i in range(min_train_size, n_test):\n",
    "        # Use training data + first i test observations for CV training\n",
    "        cv_train_series = full_data['Spread'].iloc[:train_end_idx + i]\n",
    "        \n",
    "        # Predict next test observation using moving average\n",
    "        cv_val_actual = test_data['Spread'].iloc[i]\n",
    "        cv_val_pred = moving_average_forecast(cv_train_series, window_length)\n",
    "        \n",
    "        mae_fold = abs(cv_val_actual - cv_val_pred)\n",
    "        cv_scores.append(mae_fold)\n",
    "    \n",
    "    return cv_scores\n",
    "\n",
    "# Test different window lengths and find optimal\n",
    "window_lengths = [2, 3, 4, 5, 6, 8, 10, 12, 15, 20, 24, 30]\n",
    "cv_results = {}\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"MOVING AVERAGE - TIME SERIES CROSS-VALIDATION RESULTS:\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"{'Window Length':<15} {'Mean CV MAE':<12} {'Std CV MAE':<12} {'# CV Folds':<10}\")\n",
    "print(f\"{'-'*70}\")\n",
    "\n",
    "for window_length in window_lengths:\n",
    "    # Perform cross-validation\n",
    "    cv_scores = moving_average_cv_on_test(test_data, train_data, window_length, min_train_size=5)\n",
    "    \n",
    "    if len(cv_scores) > 0:\n",
    "        mean_cv_mae = np.mean(cv_scores)\n",
    "        std_cv_mae = np.std(cv_scores)\n",
    "        n_folds = len(cv_scores)\n",
    "        \n",
    "        cv_results[window_length] = {\n",
    "            'mean_mae': mean_cv_mae,\n",
    "            'std_mae': std_cv_mae,\n",
    "            'n_folds': n_folds,\n",
    "            'cv_scores': cv_scores\n",
    "        }\n",
    "        \n",
    "        print(f\"{window_length:<15} {mean_cv_mae:<12.4f} {std_cv_mae:<12.4f} {n_folds:<10}\")\n",
    "    else:\n",
    "        print(f\"{window_length:<15} {'No CV folds':<12} {'N/A':<12} {'0':<10}\")\n",
    "\n",
    "# Find optimal window length\n",
    "if cv_results:\n",
    "    optimal_window = min(cv_results.keys(), key=lambda x: cv_results[x]['mean_mae'])\n",
    "    optimal_mae = cv_results[optimal_window]['mean_mae']\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"OPTIMAL MOVING AVERAGE CONFIGURATION:\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Optimal window length: {optimal_window}\")\n",
    "    print(f\"Optimal CV MAE: {optimal_mae:.4f}\")\n",
    "    print(f\"Optimal CV MAE Std: {cv_results[optimal_window]['std_mae']:.4f}\")\n",
    "    print(f\"Number of CV folds: {cv_results[optimal_window]['n_folds']}\")\n",
    "    \n",
    "    # Test final forecast performance with optimal window\n",
    "    def moving_average_test_forecast(train_data, test_data, window_length):\n",
    "        \"\"\"\n",
    "        Generate moving average forecasts for entire test set\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        \n",
    "        # Start with training data\n",
    "        current_series = train_data['Spread'].copy()\n",
    "        \n",
    "        for i in range(len(test_data)):\n",
    "            # Forecast next value using moving average\n",
    "            pred = moving_average_forecast(current_series, window_length)\n",
    "            predictions.append(pred)\n",
    "            \n",
    "            # Add actual value to series for next prediction (rolling forecast)\n",
    "            current_series = pd.concat([current_series, test_data['Spread'].iloc[i:i+1]])\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    # Generate forecasts with optimal window\n",
    "    optimal_predictions = moving_average_test_forecast(train_data, test_data, optimal_window)\n",
    "    test_mae_optimal = mean_absolute_error(test_data['Spread'], optimal_predictions)\n",
    "    \n",
    "    print(f\"\\nTest Set Performance with Optimal Window:\")\n",
    "    print(f\"Test MAE: {test_mae_optimal:.4f}\")\n",
    "    \n",
    "    # Show detailed CV results for optimal window\n",
    "    print(f\"\\nDetailed CV Results for Optimal Window (Length = {optimal_window}):\")\n",
    "    optimal_cv_scores = cv_results[optimal_window]['cv_scores']\n",
    "    print(f\"Individual CV MAE scores: {[f'{score:.4f}' for score in optimal_cv_scores[:10]]}\")\n",
    "    if len(optimal_cv_scores) > 10:\n",
    "        print(f\"... and {len(optimal_cv_scores) - 10} more\")\n",
    "    \n",
    "    # Performance comparison table\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"PERFORMANCE RANKING (by CV MAE):\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"{'Rank':<5} {'Window':<8} {'CV MAE':<10} {'Std MAE':<10} {'# Folds':<8}\")\n",
    "    print(f\"{'-'*70}\")\n",
    "    \n",
    "    # Sort by CV MAE\n",
    "    sorted_results = sorted(cv_results.items(), key=lambda x: x[1]['mean_mae'])\n",
    "    \n",
    "    for rank, (window, results) in enumerate(sorted_results, 1):\n",
    "        print(f\"{rank:<5} {window:<8} {results['mean_mae']:<10.4f} \"\n",
    "              f\"{results['std_mae']:<10.4f} {results['n_folds']:<8}\")\n",
    "    \n",
    "    # Additional analysis\n",
    "    best_3_windows = [item[0] for item in sorted_results[:3]]\n",
    "    worst_3_windows = [item[0] for item in sorted_results[-3:]]\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"ANALYSIS SUMMARY:\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Best 3 window lengths: {best_3_windows}\")\n",
    "    print(f\"Worst 3 window lengths: {worst_3_windows}\")\n",
    "    \n",
    "    best_mae = sorted_results[0][1]['mean_mae']\n",
    "    worst_mae = sorted_results[-1][1]['mean_mae']\n",
    "    improvement = ((worst_mae - best_mae) / worst_mae) * 100\n",
    "    \n",
    "    print(f\"Performance improvement (best vs worst): {improvement:.2f}%\")\n",
    "    print(f\"Best CV MAE: {best_mae:.4f}\")\n",
    "    print(f\"Worst CV MAE: {worst_mae:.4f}\")\n",
    "\n",
    "else:\n",
    "    print(\"No valid CV results obtained. Check data size and parameters.\")\n",
    "\n",
    "# Function to get CV results for a specific window length\n",
    "def get_ma_cv_performance(window_length):\n",
    "    \"\"\"\n",
    "    Get CV performance for a specific moving average window length\n",
    "    \"\"\"\n",
    "    if window_length in cv_results:\n",
    "        result = cv_results[window_length]\n",
    "        print(f\"\\nMoving Average (Window={window_length}) CV Performance:\")\n",
    "        print(f\"Mean CV MAE: {result['mean_mae']:.4f}\")\n",
    "        print(f\"Std CV MAE: {result['std_mae']:.4f}\")\n",
    "        print(f\"Number of CV folds: {result['n_folds']}\")\n",
    "        return result['mean_mae']\n",
    "    else:\n",
    "        print(f\"Window length {window_length} not tested. Available: {list(cv_results.keys())}\")\n",
    "        return None\n",
    "\n",
    "# Example usage:\n",
    "# ma_performance = get_ma_cv_performance(5)  # Get performance for window=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71e623d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
